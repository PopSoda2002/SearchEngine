{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment aims to introduce students to working with the BEIR dataset for information retrieval tasks. Students will:\n",
    "\n",
    "- Understand the structure of the BEIR dataset and preprocess the data.\n",
    "- Implement a system to encode queries and documents using embeddings.\n",
    "- Calculate similarity scores to rank documents based on relevance.\n",
    "- Evaluate the system's performance using metrics like Mean Average Precision (MAP).\n",
    "- Modify and fine-tune models for better retrieval results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First start by looking into the Dataset and understanding its structure.\n",
    "This will help you understand how the dataset is formed, which will be useful in the later stages of the Assignment\n",
    "\n",
    "https://huggingface.co/datasets/BeIR/nfcorpus\n",
    "\n",
    "https://huggingface.co/datasets/BeIR/nfcorpus-qrels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment consists of two key tasks: Ranking Documents and Fine-Tuning the Sentence Transformer Model. \n",
    "Students will be graded based on their implementation and their written report.\n",
    "\n",
    "Mention the team/Individual contributions as a part of the report..!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Ranking Documents Report (10 Pts)**\n",
    "\n",
    "Students must analyze which encoding methods performed best for document ranking.\n",
    "\n",
    "What to include in your report:\n",
    "    \n",
    "**1.1 Comparison of Encoding Methods**\n",
    "\n",
    "Compare GloVe embeddings vs. Sentence Transformer embeddings.\n",
    "Which method ranked documents better?\n",
    "Did the top-ranked documents make sense?\n",
    "How does cosine similarity behave with different embeddings?\n",
    "\n",
    "**1.2 Observations on Cosine Similarity & Ranking**\n",
    "\n",
    "Did the ranking appear meaningful?\n",
    "Were there cases where documents that should be highly ranked were not?\n",
    "What are possible explanations for incorrect rankings?\n",
    "\n",
    "**1.3 Possible Improvements**\n",
    "\n",
    "What can be done to improve document ranking?\n",
    "Would a different distance metric (e.g., Euclidean, Manhattan) help?\n",
    "Would preprocessing the queries or documents (e.g., removing stopwords) improve ranking?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Fine-Tuning Report (15 Pts)**\n",
    "\n",
    "After fine-tuning, students must compare different training approaches and reflect on their findings.\n",
    "\n",
    "What to include in your report:\n",
    "    \n",
    "**2.1 Comparison of Different Training Strategies**\n",
    "\n",
    "[anchor, positive] vs [anchor, positive, negative].\n",
    "Which approach seemed to improve ranking?\n",
    "How did the model behave differently?\n",
    "\n",
    "**2.2 Impact on MAP Score**\n",
    "\n",
    "Did fine-tuning improve or hurt the Mean Average Precision (MAP) score?\n",
    "If MAP decreased, why might that be?\n",
    "Is fine-tuning always necessary for retrieval models?\n",
    "\n",
    "**2.3 Observations on Training Loss & Learning Rate**\n",
    "\n",
    "Did the loss converge?\n",
    "Was the learning rate too high or too low?\n",
    "How did freezing/unfreezing layers impact training?\n",
    "\n",
    "**Future Improvements**\n",
    "\n",
    "Would training with more negatives help?\n",
    "Would changing the loss function (e.g., using Softmax Loss) improve performance?\n",
    "Could increasing the number of epochs lead to a better model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d4ade3ddc094699ac3c7452d18dc07b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create your API token from your Hugging Face Account. Make sure to save it in text file or notepad for future use.\n",
    "# Will need to add it once per section\n",
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "\n",
    "class TextSimilarityModel:\n",
    "    def __init__(self, corpus_name, rel_name, model_name='all-MiniLM-L6-v2', top_k=10):\n",
    "        \"\"\"\n",
    "        Initialize the model with datasets and pre-trained sentence transformer.\n",
    "        \"\"\"\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.corpus_name = corpus_name\n",
    "        self.rel_name = rel_name\n",
    "        self.top_k = top_k\n",
    "        self.load_data()\n",
    "\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load and filter datasets based on test queries and documents.\n",
    "        \"\"\"\n",
    "        # Load query and document datasets\n",
    "        dataset_queries = load_dataset(self.corpus_name, \"queries\")\n",
    "        dataset_docs = load_dataset(self.corpus_name, \"corpus\")\n",
    "\n",
    "        # Extract queries and documents\n",
    "        self.queries = dataset_queries[\"queries\"][\"text\"]\n",
    "        self.query_ids = dataset_queries[\"queries\"][\"_id\"]\n",
    "        self.documents = dataset_docs[\"corpus\"][\"text\"]\n",
    "        self.document_ids = dataset_docs[\"corpus\"][\"_id\"]\n",
    "\n",
    "                \n",
    "        # Filter queries and documents and build relevant queries and documents mapping based on test set\n",
    "        test_qrels = load_dataset(self.rel_name)[\"test\"]\n",
    "        self.filtered_test_query_ids = set(test_qrels[\"query-id\"])\n",
    "        self.filtered_test_doc_ids = set(test_qrels[\"corpus-id\"])\n",
    "\n",
    "        self.test_queries = [q for qid, q in zip(self.query_ids, self.queries) if qid in self.filtered_test_query_ids]\n",
    "        self.test_query_ids = [qid for qid in self.query_ids if qid in self.filtered_test_query_ids]\n",
    "        self.test_documents = [doc for did, doc in zip(self.document_ids, self.documents) if did in self.filtered_test_doc_ids]\n",
    "        self.test_document_ids = [did for did in self.document_ids if did in self.filtered_test_doc_ids]\n",
    "\n",
    "        self.test_query_id_to_relevant_doc_ids = {qid: [] for qid in self.test_query_ids}\n",
    "        for qid, doc_id in zip(test_qrels[\"query-id\"], test_qrels[\"corpus-id\"]):\n",
    "            if qid in self.test_query_id_to_relevant_doc_ids:\n",
    "                self.test_query_id_to_relevant_doc_ids[qid].append(doc_id)\n",
    "                \n",
    "        ## Code Below this is used for creating the training set \n",
    "        # Build query and document id to text mapping\n",
    "        self.query_id_to_text = {query_id:query for query_id, query in zip(self.query_ids, self.queries)}\n",
    "        self.document_id_to_text = {document_id:document for document_id, document in zip(self.document_ids, self.documents)}\n",
    "\n",
    "        # Build relevant queries and documents mapping based on train set\n",
    "        train_qrels = load_dataset(self.rel_name)[\"train\"]\n",
    "        self.train_query_id_to_relevant_doc_ids = {qid: [] for qid in train_qrels[\"query-id\"]}\n",
    "\n",
    "        for qid, doc_id in zip(train_qrels[\"query-id\"], train_qrels[\"corpus-id\"]):\n",
    "            if qid in self.train_query_id_to_relevant_doc_ids:\n",
    "                # Append the document ID to the relevant doc mapping\n",
    "                self.train_query_id_to_relevant_doc_ids[qid].append(doc_id)\n",
    "        \n",
    "        # Filter queries and documents and build relevant queries and documents mapping based on validation set  \n",
    "        #TODO Put your code here. \n",
    "        ###########################################################################\n",
    "       \n",
    "        ###########################################################################\n",
    "        \n",
    "\n",
    "    #Task 1: Encode Queries and Documents (10 Pts)\n",
    "\n",
    "    def encode_with_glove(self, glove_file_path: str, sentences: list[str]) -> list[np.ndarray]:\n",
    "\n",
    "        \"\"\"\n",
    "        # Inputs:\n",
    "            - glove_file_path (str): Path to the GloVe embeddings file (e.g., \"glove.6B.50d.txt\").\n",
    "            - sentences (list[str]): A list of sentences to encode.\n",
    "\n",
    "        # Output:\n",
    "            - list[np.ndarray]: A list of sentence embeddings \n",
    "            \n",
    "        (1) Encodes sentences by averaging GloVe 50d vectors of words in each sentence.\n",
    "        (2) Return a sequence of embeddings of the sentences.\n",
    "        Download the glove vectors from here. \n",
    "        https://nlp.stanford.edu/data/glove.6B.zip\n",
    "        Handle unknown words by using zero vectors\n",
    "        \"\"\"\n",
    "        #TODO Put your code here. \n",
    "        ###########################################################################\n",
    "\n",
    "        # Load GloVe embeddings\n",
    "        if not hasattr(self, 'glove_embeddings'):\n",
    "            self.glove_embeddings = {}\n",
    "            with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    values = line.split()\n",
    "                    word = values[0]\n",
    "                    vector = np.array(values[1:], dtype='float32')\n",
    "                    self.glove_embeddings[word] = vector\n",
    "\n",
    "\n",
    "        # Encode sentences\n",
    "        dim = int(glove_file_path.split('.')[-2][:-1]) # dimension of the glove embeddings\n",
    "        sentence_embeddings = []\n",
    "        for sentence in sentences:\n",
    "            word_embeddings = [self.glove_embeddings.get(word, np.zeros(dim)) for word in sentence.split()]\n",
    "            sentence_embedding = np.mean(word_embeddings, axis=0)\n",
    "            sentence_embeddings.append(sentence_embedding)\n",
    "\n",
    "        return sentence_embeddings\n",
    "\n",
    "        ###########################################################################\n",
    "\n",
    "    #Task 2: Calculate Cosine Similarity and Rank Documents (20 Pts)\n",
    "    \n",
    "    def rank_documents(self, encoding_method: str = 'sentence_transformer') -> None:\n",
    "        \"\"\"\n",
    "         # Inputs:\n",
    "            - encoding_method (str): The method used for encoding queries/documents. \n",
    "                             Options: ['glove', 'sentence_transformer'].\n",
    "\n",
    "        # Output:\n",
    "            - None (updates self.query_id_to_ranked_doc_ids with ranked document IDs).\n",
    "    \n",
    "        (1) Compute cosine similarity between each document and the query\n",
    "        (2) Rank documents for each query and save the results in a dictionary \"query_id_to_ranked_doc_ids\" \n",
    "            This will be used in \"mean_average_precision\"\n",
    "            Example format {2: [125, 673], 35: [900, 822]}\n",
    "        \"\"\"\n",
    "        if encoding_method == 'glove':\n",
    "            query_embeddings = self.encode_with_glove(\"glove.6B.50d.txt\", self.queries)\n",
    "            document_embeddings = self.encode_with_glove(\"glove.6B.50d.txt\", self.documents)\n",
    "        elif encoding_method == 'sentence_transformer':\n",
    "            query_embeddings = self.model.encode(self.queries)\n",
    "            document_embeddings = self.model.encode(self.documents)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid encoding method. Choose 'glove' or 'sentence_transformer'.\")\n",
    "        \n",
    "        #TODO Put your code here.\n",
    "        ###########################################################################\n",
    "         # define a dictionary to store the ranked documents for each query\n",
    "        self.query_id_to_ranked_doc_ids = {}\n",
    "        for query_id, query_embedding in zip(self.query_ids, query_embeddings):\n",
    "            # Compute cosine similarity between the query and all documents\n",
    "            cosine_similarities = cosine_similarity([query_embedding], document_embeddings)[0]\n",
    "\n",
    "            # Rank documents based on cosine similarity\n",
    "            sorted_doc_indices = np.argsort(cosine_similarities)[::-1][:self.top_k]\n",
    "            self.query_id_to_ranked_doc_ids[query_id] = [self.document_ids[i] for i in sorted_doc_indices]\n",
    "      \n",
    "        ###########################################################################\n",
    "\n",
    "    @staticmethod\n",
    "    def average_precision(relevant_docs: list[str], candidate_docs: list[str]) -> float:\n",
    "        \"\"\"\n",
    "        # Inputs:\n",
    "            - relevant_docs (list[str]): A list of document IDs that are relevant to the query.\n",
    "            - candidate_docs (list[str]): A list of document IDs ranked by the model.\n",
    "\n",
    "        # Output:\n",
    "            - float: The average precision score\n",
    "    \n",
    "        Compute average precision for a single query.\n",
    "        \"\"\"\n",
    "        y_true = [1 if doc_id in relevant_docs else 0 for doc_id in candidate_docs]\n",
    "        precisions = [np.mean(y_true[:k+1]) for k in range(len(y_true)) if y_true[k]]\n",
    "        return np.mean(precisions) if precisions else 0\n",
    "\n",
    "    #Task 3: Calculate Evaluate System Performance (10 Pts)\n",
    "    \n",
    "    def mean_average_precision(self) -> float:\n",
    "        \"\"\"\n",
    "        # Inputs:\n",
    "            - None (uses ranked documents stored in self.query_id_to_ranked_doc_ids).\n",
    "\n",
    "        # Output:\n",
    "            - float: The MAP score, computed as the mean of all average precision scores.\n",
    "    \n",
    "        (1) Compute mean average precision for all queries using the \"average_precision\" function.\n",
    "        (2) Compute the mean of all average precision scores\n",
    "        Return the mean average precision score\n",
    "        \n",
    "        reference: https://www.evidentlyai.com/ranking-metrics/mean-average-precision-map\n",
    "        https://towardsdatascience.com/map-mean-average-precision-might-confuse-you-5956f1bfa9e2\n",
    "        \"\"\"\n",
    "         #TODO Put your code here. \n",
    "        ###########################################################################\n",
    "\n",
    "        # compute average precision for each query in the test set\n",
    "        average_precision_scores = []\n",
    "        for query_id in self.test_query_id_to_relevant_doc_ids:\n",
    "            relevant_docs = set(self.test_query_id_to_relevant_doc_ids[query_id])\n",
    "            ranked_doc_ids = self.query_id_to_ranked_doc_ids.get(query_id, [])\n",
    "            average_precision_scores.append(self.average_precision(relevant_docs, ranked_doc_ids))\n",
    "\n",
    "        return np.mean(average_precision_scores)\n",
    "        ###########################################################################\n",
    "    \n",
    "    #Task 4: Ranking the Top 10 Documents based on Similarity Scores (10 Pts)\n",
    "   \n",
    "    def show_ranking_documents(self, example_query: str) -> None:\n",
    "        \n",
    "        \"\"\"\n",
    "        # Inputs:\n",
    "            - example_query (str): A query string for which top-ranked documents should be displayed.\n",
    "\n",
    "        # Output:\n",
    "            - None (prints the ranked documents along with similarity scores).\n",
    "        \n",
    "        (1) rank documents with given query with cosine similarity scores\n",
    "        (2) prints the top 10 results along with its similarity score.\n",
    "        \n",
    "        \"\"\"\n",
    "        #TODO Put your code here. \n",
    "        query_embedding = self.model.encode(example_query)\n",
    "        document_embeddings = self.model.encode(self.documents)\n",
    "        ###########################################################################\n",
    "        # Compute cosine similarity between the query and all documents\n",
    "        cosine_similarities = cosine_similarity([query_embedding], document_embeddings)[0]\n",
    "\n",
    "        # Rank documents based on cosine similarity\n",
    "        sorted_doc_indices = np.argsort(cosine_similarities)[::-1][:10]\n",
    "        for i in sorted_doc_indices:\n",
    "            print(\"Document ID: {}, Similarity Score: {}, Document Text: {},\".format(self.document_ids[i], cosine_similarities[i], self.documents[i]))\n",
    "        ###########################################################################\n",
    "      \n",
    "    #Task 5:Fine tune the sentence transformer model (25 Pts)\n",
    "    # Students are not graded on achieving a high MAP score. \n",
    "    # The key is to show understanding, experimentation, and thoughtful analysis.\n",
    "    \n",
    "    def fine_tune_model(self, batch_size: int = 32, num_epochs: int = 3, save_model_path: str = \"finetuned_senBERT\") -> None:\n",
    "\n",
    "        \"\"\"\n",
    "        Fine-tunes the model using MultipleNegativesRankingLoss.\n",
    "        (1) Prepare training examples from `self.prepare_training_examples()`\n",
    "        (2) Experiment with [anchor, positive] vs [anchor, positive, negative]\n",
    "        (3) Define a loss function (`MultipleNegativesRankingLoss`)\n",
    "        (4) Freeze all model layers except the final layers\n",
    "        (5) Train the model with the specified learning rate\n",
    "        (6) Save the fine-tuned model\n",
    "        \"\"\"\n",
    "        #TODO Put your code here.\n",
    "        ###########################################################################\n",
    "\n",
    "        ###########################################################################\n",
    "\n",
    "    # Take a careful look into how the training set is created\n",
    "    def prepare_training_examples(self) -> list[InputExample]:\n",
    "\n",
    "        \"\"\"\n",
    "        Prepares training examples from the training data.\n",
    "        # Inputs:\n",
    "            - None (uses self.train_query_id_to_relevant_doc_ids to create training pairs).\n",
    "\n",
    "         # Output:\n",
    "            Output: - list[InputExample]: A list of training samples containing [anchor, positive] or [anchor, positive, negative].\n",
    "            \n",
    "        \"\"\"\n",
    "        train_examples = []\n",
    "        for qid, doc_ids in self.train_query_id_to_relevant_doc_ids.items():\n",
    "            for doc_id in doc_ids:\n",
    "                anchor = self.query_id_to_text[qid]\n",
    "                positive = self.document_id_to_text[doc_id]\n",
    "                # TODO: Select random negative examples that are not relevant to the query.\n",
    "                # TODO: Create list[InputExample] of type [anchor, positive, negative]\n",
    "                \n",
    "                \n",
    "\n",
    "                train_examples.append(InputExample(texts=[anchor, positive]))\n",
    "\n",
    "        return train_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and use the model\n",
    "model = TextSimilarityModel(\"BeIR/nfcorpus\", \"BeIR/nfcorpus-qrels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking with sentence_transformer...\n",
      "Mean Average Precision: 0.4586185117321782\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compare the outputs \n",
    "print(\"Ranking with sentence_transformer...\")\n",
    "model.rank_documents(encoding_method='sentence_transformer')\n",
    "map_score = model.mean_average_precision()\n",
    "print(\"Mean Average Precision:\", map_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking with glove...\n",
      "Mean Average Precision: 0.05097854931446263\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compare the outputs \n",
    "print(\"Ranking with glove...\")\n",
    "model.rank_documents(encoding_method='glove')\n",
    "map_score = model.mean_average_precision()\n",
    "print(\"Mean Average Precision:\", map_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: MED-2439, Similarity Score: 0.6946136951446533, Document Text: While many factors are involved in the etiology of cancer, it has been clearly established that diet significantly impacts oneâ€™s risk for this disease. More recently, specific food components have been identified which are uniquely beneficial in mitigating the risk of specific cancer subtypes. Plant sterols are well known for their effects on blood cholesterol levels, however research into their potential role in mitigating cancer risk remains in its infancy. As outlined in this review, the cholesterol modulating actions of plant sterols may overlap with their anti-cancer actions. Breast cancer is the most common malignancy affecting women and there remains a need for effective adjuvant therapies for this disease, for which plant sterols may play a distinctive role.,\n",
      "Document ID: MED-2434, Similarity Score: 0.672288179397583, Document Text: The specific role of dietary fat in breast cancer progression is unclear, although a low-fat diet was associated with decreased recurrence of estrogen receptor alpha negative (ER(-)) breast cancer. ER(-) basal-like MDA-MB-231 and MDA-MB-436 breast cancer cell lines contained a greater number of cytoplasmic lipid droplets compared to luminal ER(+) MCF-7 cells. Therefore, we studied lipid storage functions in these cells. Both triacylglycerol and cholesteryl ester (CE) concentrations were higher in the ER(-) cells, but the ability to synthesize CE distinguished the two types of breast cancer cells. Higher baseline, oleic acid- and LDL-stimulated CE concentrations were found in ER(-) compared to ER(+) cells. The differences corresponded to greater mRNA and protein levels of acyl-CoA:cholesterol acyltransferase 1 (ACAT1), higher ACAT activity, higher caveolin-1 protein levels, greater LDL uptake, and lower de novo cholesterol synthesis in ER(-) cells. Human LDL stimulated proliferation of ER(-) MDA-MB-231 cells, but had little effect on proliferation of ER(+) MCF-7 cells. The functional significance of these findings was demonstrated by the observation that the ACAT inhibitor CP-113,818 reduced proliferation of breast cancer cells, and specifically reduced LDL-induced proliferation of ER(-) cells. Taken together, our studies show that a greater ability to take up, store and utilize exogenous cholesterol confers a proliferative advantage to basal-like ER(-) breast cancer cells. Differences in lipid uptake and storage capability may at least partially explain the differential effect of a low-fat diet on human breast cancer recurrence.,\n",
      "Document ID: MED-2440, Similarity Score: 0.6472535133361816, Document Text: Purpose To further clarify the relationship between total cholesterol and cancer, which remains unclear. Methods We prospectively examined the association between total cholesterol and site-specific and all-cancer incidence among 1,189,719 Korean adults enrolled in the National Health Insurance Corporation who underwent a standardized biennial medical examination in 1992 to 1995 and were observed for 14 years until cancer diagnosis or death. Results Over follow-up, 53,944 men and 24,475 women were diagnosed with a primary cancer. Compared with levels less than 160 mg/dL, high total cholesterol (â‰¥ 240 mg/dL) was positively associated with prostate cancer (hazard ratio [HR], 1.24; 95% CI, 1.07 to 1.44; P trend = .001) and colon cancer (HR, 1.12; 95% CI, 1.00 to 1.25; P trend = .05) in men and breast cancer in women (HR, 1.17; 95% CI, 1.03 to 1.33; P trend = .03). Higher total cholesterol was associated with a lower incidence of liver cancer (men: HR, 0.42; 95% CI, 0.38 to 0.45; P trend < .001; women: HR, 0.32; 95% CI, 0.27 to 0.39; P trend < .001), stomach cancer (men: HR, 0.87; 95% CI, 0.82 to 0.93; P trend â‰¤ .001; women: HR, 0.86; 95% CI, 0.77 to 0.97; P trend = .06), and, in men, lung cancer (HR, 0.89; 95% CI, 0.82 to 0.96; P trend < .001). Results for liver cancer were slightly attenuated after additional adjustment for liver enzyme levels and hepatitis B surface antigen status (men: HR, 0.60; P trend < .001; women: HR, 0.46; P trend = .003) and exclusion of the first 10 years of follow-up (men: HR, 0.59; P trend < .001; women: HR, 0.44; P trend < .001). Total cholesterol was inversely associated with all-cancer incidence in both men (HR, 0.84; 95% CI, 0.81 to 0.86; P trend < .001) and women (HR, 0.91; 95% CI, 0.87 to 0.95; P trend < .001), but these associations were attenuated after excluding incident liver cancers (men: HR, 0.95; P trend < .001; women: HR, 0.98; P trend = .32). Conclusion In this large prospective study, we found that total cholesterol was associated with the risk of several different cancers, although these relationships differed markedly by cancer site.,\n",
      "Document ID: MED-2427, Similarity Score: 0.5877291560173035, Document Text: Lipid rafts/caveolae are membrane platforms for signaling molecules that regulate various cellular functions, including cell survival. To better understand the role of rafts in tumor progression and therapeutics, we investigated the effect of raft disruption on cell viability and compared raft levels in human cancer cell lines versus their normal counterparts. Here, we report that cholesterol depletion using methyl-Î² cyclodextrin caused anoikis-like apoptosis, which in A431 cells involved decreased raft levels, Bcl-xL down-regulation, caspase-3 activation, and Akt inactivation regardless of epidermal growth factor receptor activation. Cholesterol repletion replenished rafts on the cell surface and restored Akt activation and cell viability. Moreover, the breast cancer and the prostate cancer cell lines contained more lipid rafts and were more sensitive to cholesterol depletion-induced cell death than their normal counterparts. These results indicate that cancer cells contain increased levels of rafts and suggest a potential use of raft-modulating agents as anti-cancer drugs.,\n",
      "Document ID: MED-2774, Similarity Score: 0.5497833490371704, Document Text: Concern has been expressed about the fact that cows' milk contains estrogens and could stimulate the growth of hormone-sensitive tumors. In this study, organic cows' milk and two commercial substitutes were digested in vitro and tested for their effects on the growth of cultures of prostate and breast cancer cells. Cows' milk stimulated the growth of LNCaP prostate cancer cells in each of 14 separate experiments, producing an average increase in growth rate of over 30%. In contrast, almond milk suppressed the growth of these cells by over 30%. Neither cows' milk nor almond milk affected the growth of MCF-7 breast cancer cells or AsPC-1 pancreatic cancer cells significantly. Soy milk increased the growth rate of the breast cancer cells. These data indicate that prostate and breast cancer patients should be cautioned about the possible promotional effects of commercial dairy products and their substitutes.,\n",
      "Document ID: MED-838, Similarity Score: 0.5406191349029541, Document Text: Docosahexaenoic acid (DHA) is an omega-3 fatty acid that comprises 22 carbons and 6 alternative double bonds in its hydrocarbon chain (22:6omega3). Previous studies have shown that DHA from fish oil controls the growth and development of different cancers; however, safety issues have been raised repeatedly about contamination of toxins in fish oil that makes it no longer a clean and safe source of the fatty acid. We investigated the cell growth inhibition of DHA from the cultured microalga Crypthecodinium cohnii (algal DHA [aDHA]) in human breast carcinoma MCF-7 cells. aDHA exhibited growth inhibition on breast cancer cells dose-dependently by 16.0% to 59.0% of the control level after 72-h incubations with 40 to 160 microM of the fatty acid. DNA flow cytometry shows that aDHA induced sub-G(1) cells, or apoptotic cells, by 64.4% to 171.3% of the control levels after incubations with 80 mM of the fatty acid for 24, 48, and 72 h. Western blot studies further show that aDHA did not modulate the expression of proapoptotic Bax protein but induced the downregulation of anti-apoptotic Bcl-2 expression time-dependently, causing increases of Bax/Bcl-2 ratio by 303.4% and 386.5% after 48- and 72-h incubations respectively with the fatty acid. Results from this study suggest that DHA from the cultured microalga is also effective in controlling cancer cell growth and that downregulation of antiapoptotic Bcl-2 is an important step in the induced apoptosis.,\n",
      "Document ID: MED-2430, Similarity Score: 0.5204560160636902, Document Text: The objective of this study was to investigate the effects of the dietary phytosterol beta-sitosterol (SIT) and the antiestrogen drug tamoxifen (TAM) on cell growth and ceramide (CER) metabolism in MCF-7 and MDA-MB-231 human breast cancer cells. The MCF-7 and MDA-MB-231 cell lines were studied as models of estrogen receptor positive and estrogen receptor negative breast cancer cells. Growth of both cell lines as determined using the sulforhodamine B assay was inhibited by treatment with 16 microM SIT but only MCF-7 cell growth was inhibited by treatment with 1 microM TAM. The combination of SIT and TAM further inhibited growth in both cell lines, most significantly in MDA-MB-231 cells. CER is a proapoptotic signal and CER levels were increased in both MCF-7 and MDA-MB-231 cells by individual treatment with SIT and TAM and the combined treatment raised cellular CER content even further. SIT and TAM raised CER levels by different means. SIT potently activated de novo CER synthesis in both MCF-7 and MDA-MB-231 cells by stimulating serine palmitoyltransferase activity; whereas TAM promoted CER accumulation in both cell types by inhibiting CER glycosylation. These results suggest that the combination regimen of dietary SIT and TAM chemotherapy may be beneficial in the management of breast cancer patients.,\n",
      "Document ID: MED-2102, Similarity Score: 0.5140798091888428, Document Text: The effects of the major human serum bile acid, glycochenodeoxycholic acid (GCDC), as well as unconjugated chenodeoxycholic acid (CDC), on the MCF-7 human breast cancer cell line have been studied in vitro under oestrogen and bile acid deprived culture conditions. GCDC increased the growth of the breast cancer cells over the range 10-300 microM. At concentrations in excess of the bile acid binding capacity of the medium cell growth was prevented. In contrast 10 microM CDC tended to reduce cell growth. Oestrogen (ER) and progesterone (PgR) receptors, pS2 and total cathepsin D were quantified by monoclonal antibody based immunoassays. Ten to 100 microM GCDC and 10 microM CDC down-regulated ER protein and this was accompanied by induction of the oestrogen-regulated proteins PgR, pS2 and possibly cathepsin D, including increased secretion of the latter two proteins into the culture medium. All these changes were quantitatively similar to those observed with 10 nM oestradiol. The bile acid effects on ER and PgR were not due to interference with the assay procedures. Cells incubated with 50 microM GCDC or 10 microM CDC had higher pmolar concentrations of the bile acids than controls. This study suggests that naturally occurring bile acids influence the growth and steroid receptor function of human breast cancer cells.,\n",
      "Document ID: MED-2437, Similarity Score: 0.5080844163894653, Document Text: BACKGROUND: Breast cancer is the most commonly diagnosed cancer among women in the United States. Extensive research has been completed to evaluate the relationship between dietary factors and breast cancer risk and survival after breast cancer; however, a summary report with clinical inference is needed. Materials and METHODS: This review summarizes the current epidemiological and clinical trial evidence relating diet to breast cancer incidence, recurrence, survival, and mortality. The review includes emerging epidemiological studies that assess risk within breast cancer subtypes as well as a summary of previous and ongoing dietary intervention trials designed to modify breast cancer risk. RESULTS: The available literature suggests that both low-fat and high-fiber diets may be weakly protective against breast cancer, whereas total energy intake and alcohol appear to be positively associated. Fiber may be weakly protective possibly through modulation of estrogen, whereas fruit and vegetable intake is not clearly associated with risk. Obesity is a risk factor for postmenopausal disease, and adult weight gain should be avoided to reduce risk. In survivors, diet has the greatest potential influence on overall mortality rather than breast cancer-specific events. CONCLUSION: Diet is modestly associated with breast cancer risk; associations appear more pronounced for postmenopausal disease, and healthy choices after diagnosis and treatment likely support longevity more so than reduced risk for recurrent disease.,\n",
      "Document ID: MED-5066, Similarity Score: 0.5012036561965942, Document Text: Context Evidence is lacking that a dietary pattern high in vegetables, fruit, and fiber and low in total fat can influence breast cancer recurrence or survival. Objective To assess whether a major increase in vegetable, fruit, and fiber intake and a decrease in dietary fat intake reduces the risk of recurrent and new primary breast cancer and all-cause mortality among women with previously treated early stage breast cancer. Design, Setting, and Participants Multi-institutional randomized controlled trial of dietary change in 3088 women previously treated for early stage breast cancer who were 18 to 70 years old at diagnosis. Women were enrolled between 1995 and 2000 and followed up through June 1, 2006. Intervention The intervention group (n=1537) was randomly assigned to receive a telephone counseling program supplemented with cooking classes and newsletters that promoted daily targets of 5 vegetable servings plus 16 oz of vegetable juice; 3 fruit servings; 30 g of fiber; and 15% to 20% of energy intake from fat. The comparison group (n=1551) was provided with print materials describing the \"5-A-Day\" dietary guidelines. Main Outcome Measures Invasive breast cancer event (recurrence or new primary) or death from any cause. Results From comparable dietary patterns at baseline, a conservative imputation analysis showed that the intervention group achieved and maintained the following statistically significant differences vs the comparison group through 4 years: servings of vegetables, +65%; fruit, +25%; fiber, +30%, and energy intake from fat, âˆ’13%. Plasma carotenoid concentrations validated changes in fruit and vegetable intake. Throughout the study, women in both groups received similar clinical care. Over the mean 7.3-year follow-up, 256 women in the intervention group (16.7%) vs 262 in the comparison group (16.9%) experienced an invasive breast cancer event (adjusted hazard ratio, 0.96; 95% confidence interval, 0.80â€“1.14; P=.63), and 155 intervention group women (10.1%) vs 160 comparison group women (10.3%) died (adjusted hazard ratio, 0.91; 95% confidence interval, 0.72â€“1.15; P=.43). No significant interactions were observed between diet group and baseline demographics, characteristics of the original tumor, baseline dietary pattern, or breast cancer treatment. Conclusion Among survivors of early stage breast cancer, adoption of a diet that was very high in vegetables, fruit, and fiber and low in fat did not reduce additional breast cancer events or mortality during a 7.3-year follow-up period. Trial Registration clinicaltrials.gov Identifier: NCT00003787,\n"
     ]
    }
   ],
   "source": [
    "model.show_ranking_documents(\"Breast Cancer Cells Feed on Cholesterol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetune all-MiniLM-L6-v2 sentence transformer model\n",
    "model.fine_tune_model(batch_size=32, num_epochs=10, save_model_path=\"finetuned_senBERT_train_v2\")  # Adjust batch size and epochs as needed\n",
    "\n",
    "model.rank_documents()\n",
    "map_score = model.mean_average_precision()\n",
    "print(\"Mean Average Precision:\", map_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
